{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "HW2_Q3.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    " # HW2 - Q3: Evaluating Robustness of Neural Networks (35 points)\n",
    "\n",
    "**Keywords**: Adversarial Robustness, FGSM/PGD Attack, Certification\n",
    "\n",
    "**About the dataset**: \\\n",
    "The [MNIST](https://en.wikipedia.org/wiki/MNIST_database) database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems.\\\n",
    "The MNIST database contains 70,000 labeled images. Each datapoint is a $28\\times 28$ pixels grayscale image.\\\n",
    "Here we will be starting off with a pre-trained 2-hidden-layer model on the full MNIST dataset.\n",
    "\n",
    "**Agenda**:\n",
    "* In this programming challenge, you will implement adversarial attack on an MNIST neural network model as well as visualize those attacks. \n",
    "* You will do this by solving the inner maximization problem using FGSM (Fast Gradient Sign Method) and PGD (Projected Gradient Descent).\n",
    "* You will then perform verification of the model using Interval-Bound -Propagation (IBP).\n",
    "\n",
    "\n",
    "**Note:**\n",
    "* It is important that you use **GPU accelaration** for this Question.\n",
    "* A note on working with GPU:\n",
    "  * Take care that whenever declaring new tensors, set `device=device` in parameters. \n",
    "  * You can also move a declared torch tensor/model to device using `.to(device)`. \n",
    "  * To move a torch model/tensor to cpu, use `.to('cpu')`\n",
    "  * Keep in mind that all the tensors/model involved in a computation have to be on the same device (CPU/GPU).\n",
    "* Run all the cells in order.\n",
    "* **Do not edit** the cells marked with !!DO NOT EDIT!!\n",
    "* Only **add your code** to cells marked with !!!! YOUR CODE HERE !!!!\n",
    "* Do not change variable names, and use the names which are suggested."
   ],
   "metadata": {
    "id": "hjRM13EAuL3g"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ],
   "metadata": {
    "id": "ZUJgYttwjmpK"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocessing"
   ],
   "metadata": {
    "id": "mXVzZFpxD8u-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# install this library\n",
    "!pip install gdown"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-17T00:13:13.660821Z",
     "iopub.execute_input": "2022-05-17T00:13:13.66111Z",
     "iopub.status.idle": "2022-05-17T00:13:42.715188Z",
     "shell.execute_reply.started": "2022-05-17T00:13:13.661079Z",
     "shell.execute_reply": "2022-05-17T00:13:42.71436Z"
    },
    "trusted": true,
    "id": "p8yCdUrCD8u-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "* We will be using a pre-trained 2-hidden layer neural network model (`nn_model`) that takes as input features vectors of size 784, and ouputs logits vector of size 10. Each of the two hidden layers are of size 1024.\n",
    "\n",
    "* This is a highly accurate model with train accuracy of approx 99.88% and test accuracy of approx 98.14%.\n",
    "\n",
    "* We will also be loading and initializing a dummy model (`test_model`) for unit testing code implementation."
   ],
   "metadata": {
    "id": "wGrcOvZu16yi"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# !!DO NOT EDIT!!\n",
    "# imports \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import requests\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import gdown\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# set hardware device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# loading the dataset full MNIST dataset\n",
    "mnist_train = datasets.MNIST(\"./data\", train=True, download=True, transform=transforms.ToTensor())\n",
    "mnist_test = datasets.MNIST(\"./data\", train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "mnist_train.data = mnist_train.data.to(device)\n",
    "mnist_test.data = mnist_test.data.to(device)\n",
    "\n",
    "mnist_train.targets = mnist_train.targets.to(device)\n",
    "mnist_test.targets = mnist_test.targets.to(device)\n",
    "\n",
    "# number of target classes\n",
    "num_classes = 10\n",
    "num_classes_test = 2 \n",
    "\n",
    "# reshape and min-max scale\n",
    "X_train =  (mnist_train.data.reshape((mnist_train.data.shape[0], -1))/255).to(device)\n",
    "y_train = mnist_train.targets\n",
    "X_test = (mnist_test.data.reshape((mnist_test.data.shape[0], -1))/255).to(device)\n",
    "y_test = mnist_test.targets\n",
    "\n",
    "\n",
    "# load pretrained and dummy model\n",
    "url_nn_model = 'https://bit.ly/3sKvyOs'\n",
    "url_models   = 'https://bit.ly/3lsVcDn'\n",
    "gdown.download(url_nn_model, 'nn_model.pt')\n",
    "gdown.download(url_models, 'models.zip')\n",
    "ZipFile(\"models.zip\").extractall(\"./\")\n",
    "\n",
    "from model import NN_Model\n",
    "from test_model import Test_Model\n",
    "nn_model = torch.load(\"./nn_model.pt\").to(device)\n",
    "print('Pretrained model (nn_model):', nn_model)\n",
    "\n",
    "test_model = Test_Model()\n",
    "print('Dummy model (test_model):', test_model)"
   ],
   "metadata": {
    "id": "qNtGmOhl1I_m",
    "execution": {
     "iopub.status.busy": "2022-05-17T00:13:57.309615Z",
     "iopub.execute_input": "2022-05-17T00:13:57.309939Z",
     "iopub.status.idle": "2022-05-17T00:14:02.322499Z",
     "shell.execute_reply.started": "2022-05-17T00:13:57.309906Z",
     "shell.execute_reply": "2022-05-17T00:14:02.319902Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this problem set you need to access the individual layers of the neural network. The below piece of code creates a list of ordered layers for each of the neural network models for easy access."
   ],
   "metadata": {
    "id": "rTj0ezmbD8u_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# This will save the linear layers of the neural network model in a ordered list\n",
    "# Eg:\n",
    "# to access weight of first layer: model_layers[0].weight\n",
    "# to access bias of first layer: model_layers[0].bias\n",
    "model_layers = [layer for layer in nn_model.children()] # for nn_model\n",
    "test_model_layers = [layer for layer in test_model.children()] # for dummy model"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-17T00:14:02.324138Z",
     "iopub.execute_input": "2022-05-17T00:14:02.324414Z",
     "iopub.status.idle": "2022-05-17T00:14:02.329428Z",
     "shell.execute_reply.started": "2022-05-17T00:14:02.324378Z",
     "shell.execute_reply": "2022-05-17T00:14:02.328507Z"
    },
    "trusted": true,
    "id": "xNsFA3dfD8vA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# !!DO NOT EDIT!!\n",
    "# utility function to plot the images\n",
    "def plot_images(X,y,yp,M,N):\n",
    "  f,ax = plt.subplots(M,N, sharex=True, sharey=True, figsize=(N,M*1.3))\n",
    "  for i in range(M):\n",
    "    for j in range(N):\n",
    "      ax[i][j].imshow(1-X[i*N+j].cpu().numpy(), cmap=\"gray\")\n",
    "      title = ax[i][j].set_title(\"Pred: {}\".format(yp[i*N+j].max(dim=0)[1]))\n",
    "      plt.setp(title, color=('g' if yp[i*N+j].max(dim=0)[1] == y[i*N+j] else 'r'))\n",
    "      ax[i][j].set_axis_off()\n",
    "    plt.tight_layout()"
   ],
   "metadata": {
    "id": "VdoXqlWbBw0C",
    "execution": {
     "iopub.status.busy": "2022-05-17T00:14:02.330901Z",
     "iopub.execute_input": "2022-05-17T00:14:02.33159Z",
     "iopub.status.idle": "2022-05-17T00:14:02.34155Z",
     "shell.execute_reply.started": "2022-05-17T00:14:02.331554Z",
     "shell.execute_reply": "2022-05-17T00:14:02.340849Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# !!DO NOT EDIT!!\n",
    "# let us visualize a few test examples\n",
    "example_data = mnist_test.data[:18]/255\n",
    "example_data_flattened  = example_data.view((example_data.shape[0], -1)).to(device) # needed for training\n",
    "example_labels = mnist_test.targets[:18].to(device)\n",
    "plot_images(example_data, example_labels, nn_model(example_data_flattened), 3, 6)"
   ],
   "metadata": {
    "id": "ulN34Vw2CAMl",
    "execution": {
     "iopub.status.busy": "2022-05-17T00:14:02.344688Z",
     "iopub.execute_input": "2022-05-17T00:14:02.345196Z",
     "iopub.status.idle": "2022-05-17T00:14:04.451211Z",
     "shell.execute_reply.started": "2022-05-17T00:14:02.34516Z",
     "shell.execute_reply": "2022-05-17T00:14:04.450501Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "qBFXtet3mnXM"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **(a) FGSM attack:**  In this part you will create a few adversarial examples using FGSM attack. Use an attack budget $\\epsilon=0.05$.  (5 points)\n",
    "In the Fast Gradient Sign Method (FGSM), the perturbation $\\delta$ on an input example (e.g. input image) $X$ is given by $\\epsilon\\cdot sign(g)$, where $g$ is the gradient of the loss function $g:=∇_\\delta \\ell(h_θ (x+δ),y)$, and $ℓ$ is the loss function, more precisely `nn.CrossEntropyLoss`. In the first timestep, this value of $\\delta$ is $0$.\n",
    "\n",
    "### **#1.** Define a function `fgsm` which takes as input the neural network model (`model`), test examples (`X`), target labels (`y`), and the attack budget (`epsilon`). Return the value of the perturbation ($\\delta$) after one gradient descent step."
   ],
   "metadata": {
    "id": "-FQ9dkXWA_Pu"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#######\n",
    "# !!! YOUR CODE HERE !!!\n",
    "\n",
    "#######"
   ],
   "metadata": {
    "id": "T6BP3I3J1Xog",
    "execution": {
     "iopub.status.busy": "2022-05-17T00:14:04.452631Z",
     "iopub.execute_input": "2022-05-17T00:14:04.453109Z",
     "iopub.status.idle": "2022-05-17T00:14:04.458844Z",
     "shell.execute_reply.started": "2022-05-17T00:14:04.45307Z",
     "shell.execute_reply": "2022-05-17T00:14:04.458133Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **#2.** Now, consider the first few examples from the training dataset which are already defined above as `example_data_flattened` and `example_labels`. Using the function `fgsm`, get the value of `delta` for these examples. Perform prediction on the modified dataset (`example_data_flattened + delta`), and construct a similar plot of images as above. You may reuse the `plot_images` function. Is the attack successful?"
   ],
   "metadata": {
    "id": "6MhfVmBES_KZ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#######\n",
    "# !!! YOUR CODE HERE !!!\n",
    "\n",
    "#######"
   ],
   "metadata": {
    "id": "unyGm4aDS-qn",
    "execution": {
     "iopub.status.busy": "2022-05-17T00:14:04.460257Z",
     "iopub.execute_input": "2022-05-17T00:14:04.460717Z",
     "iopub.status.idle": "2022-05-17T00:14:05.786786Z",
     "shell.execute_reply.started": "2022-05-17T00:14:04.460667Z",
     "shell.execute_reply": "2022-05-17T00:14:05.786031Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ],
   "metadata": {
    "id": "n2DpSB3_-1xB"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **(b) PGD attack:**  In this part you will create a few adversarial examples using PGD attack. Use an attack budget $\\epsilon=0.05$.  (10 points)\n",
    "\n",
    "**Note:** For the Projected Gradient Descent (PGD) attack, you create an adversarial example by iteratively performing gradient descent with a fixed step size $\\alpha$. The update rule is: $δ:=P(δ+α∇_δ ℓ(h_θ(x+δ),y))$, where $δ$ is the perturbation, $θ$ are the frozen DNN parameters, $x$ and $y$ is the training example and its ground truth label respectively, $h_θ$ is the hypothesis function, $ℓ$ denotes the loss function, and $P$ denotes the projection onto a norm ball ($l_\\infty, l_1, l_2$, etc.) of interest. For $l_\\infty$ ball, this just means clamping the value of $\\delta$ between $-\\epsilon$ and $\\epsilon$.\n",
    "\n",
    "### **#1.** Instead of using FGSM, now use Projected Gradient Descent (PGD) with projection on $l_\\infty$ ball for the attack. Define a function `pgd` that takes as input the neural network model (`model`), training examples (`X`), target labels (`y`), step size (`alpha`), attack budget (`epsilon`), and number of iterations (`num_iter`). Return the perturbation ($\\delta$) after `num_iter` gradient descent steps. "
   ],
   "metadata": {
    "id": "8mSRPCJsWCmQ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#######\n",
    "# !!! YOUR CODE HERE !!!\n",
    "\n",
    "#######"
   ],
   "metadata": {
    "id": "TqSbm39KQ7nv",
    "execution": {
     "iopub.status.busy": "2022-05-17T00:14:05.787794Z",
     "iopub.execute_input": "2022-05-17T00:14:05.788125Z",
     "iopub.status.idle": "2022-05-17T00:14:05.796369Z",
     "shell.execute_reply.started": "2022-05-17T00:14:05.78808Z",
     "shell.execute_reply": "2022-05-17T00:14:05.795611Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **#2.** Now use the PGD attack for the examples from `example_data_flattened`. Use `alpha=1000`, `num_iter=1000`, and create a similar plot as before. Is the attack successful?\n",
    "\n",
    "### The value of `alpha` is large because the neural network model is pretrained and is therefore at the local minima. The value of gradients here is extremely small, and we therefore need a huge value of step size to have any hope of moving out of the local minima."
   ],
   "metadata": {
    "id": "mAO05kUnYLUF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#######\n",
    "# !!! YOUR CODE HERE !!!\n",
    "\n",
    "#######"
   ],
   "metadata": {
    "id": "IVapJ4aaYJvW",
    "execution": {
     "iopub.status.busy": "2022-05-17T00:14:05.79842Z",
     "iopub.execute_input": "2022-05-17T00:14:05.798692Z",
     "iopub.status.idle": "2022-05-17T00:14:08.011226Z",
     "shell.execute_reply.started": "2022-05-17T00:14:05.798642Z",
     "shell.execute_reply": "2022-05-17T00:14:08.010579Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ],
   "metadata": {
    "id": "VLPLU5JbAs6G"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **(c)**  Use FGSM and PGD to create adversarial examples using the complete test dataset. Create the datasets with different values of `epsilon: [0, 0.02, 0.04, 0.06, 0.08, 0.1, 0.12, 0.14, 0.16, 0.18, 0.2]`. For each of the dataset created with different `epsilon` values and attack type, get the model accuracies. Plot a (single) graph of accuracy vs. epsilon for both attack types. Note that `epsilon=0` means no attack, so you can just get accuracy on the original dataset. (10 points)\n",
    "\n",
    "#### <font color=\"red\">It is important that you use **GPU accelaration** for this part.</font>\n"
   ],
   "metadata": {
    "id": "PjC-rDNEaCqC"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#######\n",
    "# !!! YOUR CODE HERE !!!\n",
    "\n",
    "#######"
   ],
   "metadata": {
    "id": "kWAQPWwAaCQA",
    "execution": {
     "iopub.status.busy": "2022-05-16T23:02:25.278575Z",
     "iopub.execute_input": "2022-05-16T23:02:25.278969Z",
     "iopub.status.idle": "2022-05-16T23:05:24.825878Z",
     "shell.execute_reply.started": "2022-05-16T23:02:25.278928Z",
     "shell.execute_reply": "2022-05-16T23:05:24.825051Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ],
   "metadata": {
    "id": "LCvAljqaj-bR"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **(d)** Use the Interval-Bound-Propagation (IBP) technique to certify robustness of the model through lower bound with a given value of epsilon. (10 points)"
   ],
   "metadata": {
    "id": "0boUhvoAj_4g"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* In this section, you will find the lower and upper bounds for each neuron of each of the linear layers of the neural network model.\n",
    "\n",
    "* Note that the initial bound is the bound of the first layer, which is the input example. For the $l_\\infty$ perturbation, the initial lower bound is simply $max(0, x-\\epsilon)$, and the  initial upper bound is $min(1, x+\\epsilon)$, for an input example $x$ (Note that each value of $x$ must lie in between 0 and 1, thats why the $min$ and $max$).\n",
    "\n",
    "* In the function, propagate the initial bound across all layers of the neural network and return a list of tuples of *pre-activation* lower and upper bound for each layer. The *pre-activation* bounds are the bound before applying ReLU activation. \n",
    "\n",
    "* Let's review a bit of the IBP bounds: let $z=Wx+b$ denote an intermediate linear layer of the model, and suppose $\\hat{l} \\le x \\le \\hat{u}, l \\le z \\le u$, we have:\\\n",
    "$l=W_+\\hat{l}+W_-\\hat{u}+b$\\\n",
    "$u=W_+\\hat{u}+W_-\\hat{l}+b$\\\n",
    "Note $l, u$ here are the *pre-activation* bounds\n",
    "\n",
    "* If a non-linear ReLU activation function $\\sigma(\\cdot)$ is applied to the layer $z=Wx+b$, then the bounds of $\\sigma(z)$ will be: $l=\\sigma(\\hat{l}), u = \\sigma(\\hat{u})$ as $\\sigma$ is a monotonically non-decreasing function. I.e. $l \\leq \\sigma(z) \\leq u$. The $l,u$ here are the *post-activation* bounds. Note, here we use $\\hat{l}$ and $\\hat{u}$ to denote the bounds of the previous layer: $\\hat{l} \\leq z \\leq \\hat{u}$. \n",
    "\n",
    "### **#1.** Define a function `bound_propagation` which takes as input an ordered list of layers of the model (`model_layers`), a feature vector (`x`), and attack budget (`epsilon`). Return a list of tuples of `pre-activation` lower and upper bound tensors for each layer. Verify that your implementation is correct by verifying the results of your function on the unit tests given below."
   ],
   "metadata": {
    "id": "6hcmHsRX1pz1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#######\n",
    "# !!! YOUR CODE HERE !!!\n",
    "\n",
    "#######"
   ],
   "metadata": {
    "id": "8p4zMIeHj-D2",
    "execution": {
     "iopub.status.busy": "2022-05-17T00:14:09.745707Z",
     "iopub.execute_input": "2022-05-17T00:14:09.746273Z",
     "iopub.status.idle": "2022-05-17T00:14:09.757123Z",
     "shell.execute_reply.started": "2022-05-17T00:14:09.746233Z",
     "shell.execute_reply": "2022-05-17T00:14:09.756385Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# !!DO NOT EDIT!!\n",
    "sample_epsilon = 0.2\n",
    "# unit test - 1\n",
    "x_1 = torch.tensor([[0.1, 0.9]], device=device)\n",
    "test_bounds_1 = bound_propagation(test_model_layers, x_1, sample_epsilon)\n",
    "assert torch.all(torch.eq(torch.round(test_bounds_1[0][0], decimals=2), torch.tensor([[0.0000, 0.7000]], device=device)))\n",
    "assert torch.all(torch.eq(torch.round(test_bounds_1[0][1], decimals=2), torch.tensor([[0.3000, 1.0000]], device=device)))\n",
    "assert torch.all(torch.eq(torch.round(test_bounds_1[1][0], decimals=2), torch.tensor([[0.0000, 1.4000, 1.2000]], device=device)))\n",
    "assert torch.all(torch.eq(torch.round(test_bounds_1[1][1], decimals=2), torch.tensor([[0.4500, 2.6000, 1.5000]], device=device)))\n",
    "assert torch.all(torch.eq(torch.round(test_bounds_1[2][0], decimals=2), torch.tensor([[ 2.6500, -0.8000,  2.1000]], device=device)))\n",
    "assert torch.all(torch.eq(torch.round(test_bounds_1[2][1], decimals=2), torch.tensor([[6.7000, 0.1000, 4.3500]], device=device)))\n",
    "\n",
    "# unit test - 2\n",
    "x_1 = torch.tensor([[0.4, 0.5]], device=device)\n",
    "test_bounds_2 = bound_propagation(test_model_layers, x_1, sample_epsilon)\n",
    "assert torch.all(torch.eq(torch.round(test_bounds_2[0][0], decimals=2), torch.tensor([[0.2000, 0.3000]], device=device)))\n",
    "assert torch.all(torch.eq(torch.round(test_bounds_2[0][1], decimals=2), torch.tensor([[0.6000, 0.7000]], device=device)))\n",
    "assert torch.all(torch.eq(torch.round(test_bounds_2[1][0], decimals=2), torch.tensor([[0.4000, 1.0000, 0.8000]], device=device)))\n",
    "assert torch.all(torch.eq(torch.round(test_bounds_2[1][1], decimals=2), torch.tensor([[1.0000, 2.6000, 1.2000]], device=device)))\n",
    "assert torch.all(torch.eq(torch.round(test_bounds_2[2][0], decimals=2), torch.tensor([[-0.2000, -0.7000,  0.4000]], device=device)))\n",
    "assert torch.all(torch.eq(torch.round(test_bounds_2[2][1], decimals=2), torch.tensor([[5.2000, 0.5000, 3.4000]], device=device)))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-17T00:14:10.195394Z",
     "iopub.execute_input": "2022-05-17T00:14:10.195952Z",
     "iopub.status.idle": "2022-05-17T00:14:10.275105Z",
     "shell.execute_reply.started": "2022-05-17T00:14:10.195913Z",
     "shell.execute_reply": "2022-05-17T00:14:10.274366Z"
    },
    "trusted": true,
    "id": "VkjsOCP0D8vD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **#2.** Let the lower and upper bounds of the final layer of the model be $l^{final}$ and $u^{final}$ respectively. Then we say that an input example $x$ has a robutness certificate $\\epsilon$ if the criteria: $l^{final}[c]-u^{final}[i]>0, \\forall i\\ne c$, where $c$ denotes the ground truth class of the input $x$. \n",
    "\n",
    "* We need to determine the maximum value of epsilon for certified robustness against an adversarial attack for a given example. We can do the same using binary search over a few values of epsilon.\n",
    "\n",
    "* Define a function `binary_search` that takes as input a sorted array of epsilon values (`epsilons`), an ordered list of neural network model layers (`model_layers`), examples (`X`), corresponding targets (`y`), the number of target classes (`num_classes`). It should return `certified_epsilons` which is a python list of the final values of epsilon certification for each example in input. You can use `None` when unable to find an epsilon value from epsilons. \n",
    "\n",
    "* Verify that your implementation is correct by verifying the results of your function on the unit tests given below."
   ],
   "metadata": {
    "id": "JoXJUcz226wm"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#######\n",
    "# !!! YOUR CODE HERE !!!\n",
    "\n",
    "#######"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-17T00:14:14.918489Z",
     "iopub.execute_input": "2022-05-17T00:14:14.918801Z",
     "iopub.status.idle": "2022-05-17T00:14:14.929109Z",
     "shell.execute_reply.started": "2022-05-17T00:14:14.918768Z",
     "shell.execute_reply": "2022-05-17T00:14:14.928219Z"
    },
    "trusted": true,
    "id": "D4hUeavjD8vD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# !!DO NOT EDIT!!\n",
    "epsilons = [x/10000 for x in range(1, 10000)]\n",
    "# unit test - 1 \n",
    "sample_X = torch.tensor([[0.1, 0.9], [0.4, 0.5]], device=device)\n",
    "sample_y = torch.tensor([0,0], device=device)\n",
    "test_epsilons = binary_search(epsilons, test_model_layers, sample_X, sample_y, num_classes_test)\n",
    "assert test_epsilons==[0.0028, 0.0067]"
   ],
   "metadata": {
    "id": "TpcsRBWYCJTm"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **#3.** Report the certified values of epsilon on the first few examples (simply run the below cell)."
   ],
   "metadata": {
    "id": "c-oAMCniCUl6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# !!DO NOT EDIT!!\n",
    "# finding epsilon for first few examples of MNIST dataset using IBP\n",
    "epsilons = [x/10000 for x in range(1, 10000)]\n",
    "X = example_data_flattened[0:2]\n",
    "y = example_labels[0:2]\n",
    "binary_search(epsilons, model_layers, X, y, num_classes)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-17T00:14:15.586575Z",
     "iopub.execute_input": "2022-05-17T00:14:15.587148Z",
     "iopub.status.idle": "2022-05-17T00:14:15.626283Z",
     "shell.execute_reply.started": "2022-05-17T00:14:15.587115Z",
     "shell.execute_reply": "2022-05-17T00:14:15.62553Z"
    },
    "trusted": true,
    "id": "WBuMbyEpD8vD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n"
   ],
   "metadata": {
    "id": "iq8PfeYWD8vD"
   }
  }
 ]
}