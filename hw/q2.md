## Effect of using Full gradient descent vs Batch Gradient Descent

* We see that batch gradient descent takes more time for converging. However, the accuracy is much higher. 
* However the time to compute one gradient is much higher in full gradient descent as compared to stochastic/batch gradient descent.
* The reason for longer convergence is that once the parameters reach in the confusion region, the stochasticity is much higher.


## Effect of different loss strategy on performance.

* We see that time taken while using Cross Entropy loss is much higher as compared to MSE Loss
* However, the accuracy is much higher with Cross Entropy loss as compared to MSE Loss.


## Effect of using linear vs. non-linear models.
* We see non-linear models have higher accuracy as compared to linear models.
* The time taken by non-linear models considerably lower as compared to their linear counterparts.

## Training time per epoch in different cases.
* Training time per update for full GD is higher as compared to SGD as the number of gradients to be computed are less. Training time per epoch however would be roughly around the same.
* Training time per epoch is much higher when using cross entropy loss as compared to MSE loss.
* Trainig time per epoch in non-linear models is comparatively lower as compared to their linear counterparts.