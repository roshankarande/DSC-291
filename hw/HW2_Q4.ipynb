{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HW2_Q4.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# HW2 - Q4: Training a robust model & Tighter Certification (10 points)\n","\n","**Keywords**: Adversarial Robustness Training\n","\n","**About the dataset**: \\\n","The [MNIST](https://en.wikipedia.org/wiki/MNIST_database) database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems.\\\n","The MNIST database contains 70,000 labeled images. Each datapoint is a $28\\times 28$ pixels grayscale image.\n","\n","**Agenda**:\n","* In this programming challenge, you will train a 2-hidden layer neural network which is robust to adversarial attacks. \n","* You will train models on adversarial examples generated using both FGSM and PGD.\n","* Finally, you will compare the robustness of a standard (non-robust) model vs. the robust models using both IBP and FastLin bound Algorithms.\n","\n","\n","**Note:**\n","* **It is recommended that you use GPU hardware accelaration for this question.**\n","* A note on working with GPU:\n","  * Take care that whenever declaring new tensors, set `device=device` in parameters. \n","  * You can also move a declared torch tensor/model to device using `.to(device)`. \n","  * To move a torch model/tensor to cpu, use `.to('cpu')`\n","  * Keep in mind that all the tensors/model involved in a computation have to be on the same device (CPU/GPU).\n","* Run all the cells in order.\n","* **Do not edit** the cells marked with !!DO NOT EDIT!!\n","* Only **add your code** to cells marked with !!!! YOUR CODE HERE !!!!\n","* Do not change variable names, and use the names which are suggested."],"metadata":{"id":"hjRM13EAuL3g"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","\n","---\n","\n"],"metadata":{"id":"ZUJgYttwjmpK"}},{"cell_type":"markdown","source":["## Preprocessing:"],"metadata":{"id":"bFWii5HbsA4r"}},{"cell_type":"code","source":["# install this library\n","!pip install gdown"],"metadata":{"id":"N1FBs7WdqrID"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !!DO NOT EDIT!!\n","# imports \n","import torch\n","import torch.nn as nn\n","import numpy as np\n","import requests\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","import matplotlib.pyplot as plt\n","from tqdm.notebook import tqdm\n","import gdown\n","from zipfile import ZipFile\n","\n","# set device\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","# loading the dataset full MNIST dataset\n","mnist_train = datasets.MNIST(\"./data\", train=True, download=True, transform=transforms.ToTensor())\n","mnist_test = datasets.MNIST(\"./data\", train=False, download=True, transform=transforms.ToTensor())\n","\n","mnist_train.data = mnist_train.data.to(device)\n","mnist_test.data = mnist_test.data.to(device)\n","\n","mnist_train.targets = mnist_train.targets.to(device)\n","mnist_test.targets = mnist_test.targets.to(device)\n","\n","# reshape and min-max scale\n","X_train =  (mnist_train.data.reshape((mnist_train.data.shape[0], -1))/255).to(device)\n","y_train = mnist_train.targets\n","X_test = (mnist_test.data.reshape((mnist_test.data.shape[0], -1))/255).to(device)\n","y_test = mnist_test.targets\n","\n","# load pretrained and dummy model\n","url_nn_model = 'https://bit.ly/3sKvyOs'\n","url_models   = 'https://bit.ly/3lsVcDn'\n","gdown.download(url_nn_model, 'nn_model.pt')\n","gdown.download(url_models, 'models.zip')\n","ZipFile(\"models.zip\").extractall(\"./\")\n","\n","from model import NN_Model\n","from test_model import Test_Model\n","nn_model = torch.load(\"./nn_model.pt\").to(device)\n","print('Pretrained model (nn_model):', nn_model)\n","\n","test_model = Test_Model()\n","print('Dummy model (test_model):', test_model)\n","\n","# This will save the linear layers of the neural network model in a ordered list\n","# Eg:\n","# to access weight of first layer: model_layers[0].weight\n","# to access bias of first layer: model_layers[0].bias\n","model_layers = [layer for layer in nn_model.children()] # for nn_model\n","test_model_layers = [layer for layer in test_model.children()] # for dummy model\n","\n","# first few examples\n","example_data = mnist_test.data[:18]/255\n","example_data_flattened  = example_data.view((example_data.shape[0], -1)).to(device) # needed for training\n","example_labels = mnist_test.targets[:18].to(device)"],"metadata":{"id":"qNtGmOhl1I_m","execution":{"iopub.status.busy":"2022-05-13T02:49:35.929583Z","iopub.execute_input":"2022-05-13T02:49:35.930120Z","iopub.status.idle":"2022-05-13T02:49:45.968342Z","shell.execute_reply.started":"2022-05-13T02:49:35.930035Z","shell.execute_reply":"2022-05-13T02:49:45.967617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","\n","---\n","\n","\n"],"metadata":{"id":"qBFXtet3mnXM"}},{"cell_type":"markdown","source":["### **Adversarial training:** In order to train robust models, the most intuitive strategy is to train on adversarial examples.\n","* The adversarial (robust) loss function is defined as:\n","$\\underset{\\theta}{minimize} \\frac{1}{|S|}\\sum_{x,y\\in S} \\underset{∥δ∥≤ϵ}{max}\\,ℓ(h_θ(x+δ),y)$, \\\n","where $\\theta$ are the learnable parameters, $S$ is the set of training examples with $x$ representing the input example and $y$ the ground truth label, $h_\\theta$ is the hypothesis (neural network model), $\\delta$ is the attack perturbation, and $\\epsilon$ is the attack budget.\n","\n","* This is also known as the min-max loss function. The gradient descent step now becomes:\\\n","$\\theta:=\\theta-\\frac{\\alpha}{|B|}\\sum_{x,y\\in B} ∇_θ \\underset{∥δ∥≤ϵ}{max}\\,ℓ(h_θ(x+δ),y)$,\\\n","where $B$ is the mini-batch and $\\alpha$ is the learning rate. \n","\n","* Now the question becomes how to find the inner term: $∇_θ \\underset{∥δ∥≤ϵ}{max}\\,ℓ(h_θ(x+δ),y)$ of the gradient descent step. For this, we can use **Danskin’s Theorem**, which states that to compute the (sub)gradient of a function containing a max term, we need to simply 1) find the maximum, and 2) compute the normal gradient evaluated at this point. It holds only when you have the exact maximum.  Note that it is not possible to solve the inner maximization problem exactly (NP-hard). However, the better job we do of solving the inner maximization problem, the closer it seems that Danskin’s theorem starts to hold. That is why we can re-use methods such as FGSM/PGD to find approximate worst case examples. In other words, we can perform the attack to find $δ^{*} = \\underset{∥δ∥≤ϵ}{argmax}ℓ(h_θ(x+δ),y)$, and then compute this term at the perturbed image: $∇_θℓ(h_θ(x+δ^{*}),y)$.\n","\n","In summary, we would create an adversarial example for each datapoint in the mini-batch and add the loss corresponding to that example to the gradient.  "],"metadata":{"id":"lu9K75vp9nI6"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"cOLLgUNtuHtk"}},{"cell_type":"markdown","source":["### **(a) Setup:** (2 points)\n","### **#1.**Get the attack functions `fgsm` and `pgd` that you defined in the HW2-Q3 (a) and (b). Modify the `pgd` function to start with random values of `delta` instead of zeros."],"metadata":{"id":"2oxntXnpDG0y"}},{"cell_type":"code","source":["#######\n","# !!! YOUR CODE HERE !!!\n","\n","#######"],"metadata":{"id":"jY92MjbEDGf9","execution":{"iopub.status.busy":"2022-05-13T03:19:05.822355Z","iopub.execute_input":"2022-05-13T03:19:05.822659Z","iopub.status.idle":"2022-05-13T03:19:05.832077Z","shell.execute_reply.started":"2022-05-13T03:19:05.822616Z","shell.execute_reply":"2022-05-13T03:19:05.831363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **#2.** Create a 2-hidden-layer neural network model in pytorch. The input should be the size of the flattened MNIST image, and output layer should be of size 10, which is the number of target labels. Each of the two hidden layers should be of size 1024 with ReLU activations between each subsequent layer except the last layer.\n","\n","You can refer to the structure of `nn_model` from HW2-Q3. It should be similar to that."],"metadata":{"id":"hrHNjl-xDm7P"}},{"cell_type":"code","source":["#######\n","# !!!! YOUR CODE HERE !!!!\n","\n","#######"],"metadata":{"id":"b0J-mW8gDmYl","execution":{"iopub.status.busy":"2022-05-13T02:50:18.438466Z","iopub.execute_input":"2022-05-13T02:50:18.439215Z","iopub.status.idle":"2022-05-13T02:50:18.446367Z","shell.execute_reply.started":"2022-05-13T02:50:18.439150Z","shell.execute_reply":"2022-05-13T02:50:18.445636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **#3.** Define a function `train_torch_model_adversarial` that takes as input an initialized torch model (`model`), batch size (`batch_size`), initialized loss (`criterion`), max number of epochs (`max_epochs`), training data (`X_train, y_train`), learning rate (`lr`), tolerance for stopping (`tolerance`), adversarial strategy (`adversarial_strategy`: `None/'fgsm'/'pgd'`), and attack budget(`epsilon`). \n","\n","### **Note**: \n","* If `adversarial_strategy` is `None`, don't train on adversarial examples.\n","* This function will return a tuple of `(model, losses)`, where `model` is the trained model, and `losses` are a list of tuple of loss logged every epoch. \n","* The only difference from the function `train_torch_model` that you wrote in HW2-Q2 (b) is that you also add gradient of adversarial example. "],"metadata":{"id":"NYq0cTMSE1Ta"}},{"cell_type":"code","source":["#######\n","# !!!! YOUR CODE HERE !!!!\n","\n","#######"],"metadata":{"id":"cJ22m_3uFwTK","execution":{"iopub.status.busy":"2022-05-13T02:50:44.597900Z","iopub.execute_input":"2022-05-13T02:50:44.598192Z","iopub.status.idle":"2022-05-13T02:50:44.610522Z","shell.execute_reply.started":"2022-05-13T02:50:44.598160Z","shell.execute_reply":"2022-05-13T02:50:44.609437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"kOiOT9BvuJmM"}},{"cell_type":"markdown","source":["### **(b) Train the model with different strategies:** (3 points)\n","###  **#1.** Train three models, one without adversarial training, one with adversarial training using `fgsm`, and last one with adversarial training using `pgd`. \n","\n","* Use attack budget `epsilon` of 0.05.\n","\n","* Use a mini-batch of size 512, and train for 20 epochs with learning rate $10^{-2}$, and early stopping tolerance of $10^{-6}$. Report the loss in each case.\n","\n","* For `pgd`, use the value of step-size `alpha=0.01` and number of iterations `num_iter=100` when training."],"metadata":{"id":"UfrTRlDjHdpm"}},{"cell_type":"code","source":["#######\n","# !!!! YOUR CODE HERE !!!!\n","\n","#######"],"metadata":{"id":"bq7g7Se_HdSM","execution":{"iopub.status.busy":"2022-05-13T03:08:29.883351Z","iopub.execute_input":"2022-05-13T03:08:29.883635Z","iopub.status.idle":"2022-05-13T03:14:57.798446Z","shell.execute_reply.started":"2022-05-13T03:08:29.883587Z","shell.execute_reply":"2022-05-13T03:14:57.797639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"Y7rOVcnCuLPX"}},{"cell_type":"markdown","source":["### **#2.**Measuring performance: In this part we will be evaluate the trained models for different test scenarios. \n"],"metadata":{"id":"ju_Ie0ouxiWD"}},{"cell_type":"markdown","source":["### Print the accuracy of each of the three trained models on the clean test dataset. "],"metadata":{"id":"k8chvaD8xiWM"}},{"cell_type":"code","source":["#######\n","# !!!! YOUR CODE HERE !!!!\n","\n","#######"],"metadata":{"execution":{"iopub.status.busy":"2022-05-13T03:24:56.567590Z","iopub.execute_input":"2022-05-13T03:24:56.568318Z","iopub.status.idle":"2022-05-13T03:24:57.880933Z","shell.execute_reply.started":"2022-05-13T03:24:56.568280Z","shell.execute_reply":"2022-05-13T03:24:57.880188Z"},"trusted":true,"id":"gKvndMOogCkm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **#3.** Using the same test dataset, perform adversarial attack to compute robust accuracy for each of the three models. Report the robust accuracy of each model for both:\n","###(a) FGSM attack\n","###(b) PGD attack\n","\n","### To create PGD attack examples, use `alpha=0.01`, `num_iter=100`."],"metadata":{"id":"ubzMsNqt-cXF"}},{"cell_type":"code","source":["#######\n","# !!!! YOUR CODE HERE !!!!\n","\n","#######"],"metadata":{"id":"4z3T3nvR_M0o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","\n","---\n","\n"],"metadata":{"id":"J0_L-VhvuMVq"}},{"cell_type":"markdown","source":["### **(c) FastLin Bound Propagation Algorithm:** In this part, we will use Fast Linear (Fast-Lin) algorithm to get lower and upper bounds for each layer of the neural network. (2.5 points)\n","\n","**Note:** See Section 3.3 and algorithm 1 in the appendix (Section D: Algorithms) from this paper: \\\n","[Weng etal, Towards Fast Computation of Certified Robustness for ReLU Networks, ICML 2018](https://arxiv.org/pdf/1804.09699.pdf)\n"],"metadata":{"id":"25EbgUjC1JRK"}},{"cell_type":"markdown","source":["### Define a function `fast_linear_bound` that takes as input an ordered list of neural network model layers (`model_layers`), a single input example (`x`), attack budget (`epsilon`). Consider the $l_\\infty$ norm ball for this activity. Return a list of tuples of `pre-activation` lower and upper bound tensors for each layer. \n","### Also keep in mind to set device when declaring new Tensors."],"metadata":{"id":"616bu2CYtWaq"}},{"cell_type":"code","source":["#######\n","# !!! YOUR CODE HERE !!!\n","\n","#######"],"metadata":{"id":"E_tpoAWpAgXL","execution":{"iopub.status.busy":"2022-05-17T00:14:17.536828Z","iopub.execute_input":"2022-05-17T00:14:17.537413Z","iopub.status.idle":"2022-05-17T00:14:17.557304Z","shell.execute_reply.started":"2022-05-17T00:14:17.537374Z","shell.execute_reply":"2022-05-17T00:14:17.556539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !!DO NOT EDIT!!\n","sample_epsilon = 0.2\n","# unit test - 1\n","x_1 = torch.tensor([[0.1, 0.9]], device=device)\n","test_bounds_1 = fast_linear_bound(test_model_layers, x_1, sample_epsilon)\n","assert torch.all(torch.eq(torch.round(test_bounds_1[0][0], decimals=2), torch.tensor([[0.0000, 0.7000]], device=device)))\n","assert torch.all(torch.eq(torch.round(test_bounds_1[0][1], decimals=2), torch.tensor([[0.3000, 1.0000]], device=device)))\n","assert torch.all(torch.eq(torch.round(test_bounds_1[1][0], decimals=2), torch.tensor([[0.0000, 1.4000, 1.2000]], device=device)))\n","assert torch.all(torch.eq(torch.round(test_bounds_1[1][1], decimals=2), torch.tensor([[0.4500, 2.6000, 1.5000]], device=device)))\n","assert torch.all(torch.eq(torch.round(test_bounds_1[2][0], decimals=2), torch.tensor([[3.100, -0.5000,  2.4000]], device=device)))\n","assert torch.all(torch.eq(torch.round(test_bounds_1[2][1], decimals=2), torch.tensor([[6.25000, -0.2000, 4.0500]], device=device)))\n","assert torch.all(torch.eq(torch.round(test_bounds_1[3][0], decimals=2), torch.tensor([[0.500, -6.75000]], device=device)))\n","assert torch.all(torch.eq(torch.round(test_bounds_1[3][1], decimals=2), torch.tensor([[0.500, -5.55000]], device=device)))"],"metadata":{"id":"I7YfBUwZsrhq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"m3Aus3g-5PjH"}},{"cell_type":"markdown","source":["### **(d) Comparison of certification strategies** (2.5 points)\n","### **#1.** Get the functions  `bound_propagation` and `binary_search` that you defined in HW3-Q2(d). Refactor the function `binary_search` so that it takes one additional input parameter: `bound_function` which represents a function that can be used to get the bounds, and modify your implementation of `binary_search` so that it uses the parameter `bound_function` for getting the bounds."],"metadata":{"id":"FC9urVZrufqW"}},{"cell_type":"code","source":["#######\n","# !!! YOUR CODE HERE !!!\n","\n","#######"],"metadata":{"id":"fvpaTyWfvNyi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **#2.** Now, consider the first example from `example_data_flattened`. For this example find the value of certified epsilon (using `binary_search`) using (1) IBP (2) Fast-Lin on the standard model that you trained in part (b). Compare your results."],"metadata":{"id":"oZy27fBPvO-s"}},{"cell_type":"code","source":["#######\n","# !!! YOUR CODE HERE !!!\n","\n","#######"],"metadata":{"id":"LXAjAGsCwNnj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **#3.** Repeat the same activity as above but use the robust model that you trained on PGD adversarial example. Compare you results."],"metadata":{"id":"RG9NJzevwld4"}},{"cell_type":"code","source":["#######\n","# !!! YOUR CODE HERE !!!\n","\n","#######"],"metadata":{"id":"BtgQUs03wxJD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","---"],"metadata":{"id":"452OpRGwD8vE"}}]}